# Click Analytics Design

## Overview

Record redirect clicks for analytics: who clicked, where from, when. Low traffic expected — simple synchronous inserts into PostgreSQL. When scaling is needed, switch to RabbitMQ (already available in infrastructure).

## Responsibility Boundaries

### Redirector (this service)

- Collects raw click data from the HTTP request
- Sends a **raw event** (url_id, IP, User-Agent string, Referer string, timestamp)
- Phase 1: direct INSERT of raw data into PG
- Phase 2: publish raw event to RabbitMQ

Redirector does NOT normalize data — no UA dictionaries, no referrer parsing, no domain extraction.

### Consumer service (separate)

- Reads raw click events (from PG or RabbitMQ queue)
- Normalizes User-Agent into dictionary
- Extracts referrer domain
- Resolves IP to geographic location (GeoIP via MaxMind)
- Manages partitions
- Builds aggregations if needed

## Database Schema

### Raw clicks table — written by redirector

```sql
CREATE SCHEMA IF NOT EXISTS analytics;

CREATE TABLE analytics.clicks (
    id              BIGINT NOT NULL,       -- Snowflake ID (generated by consumer)
    url_id          BIGINT NOT NULL,
    ip              INET NOT NULL,
    user_agent      TEXT,
    referrer        TEXT,
    created_at      TIMESTAMP NOT NULL DEFAULT now(),  -- always UTC
    PRIMARY KEY (id, created_at)
) PARTITION BY RANGE (created_at);

CREATE INDEX idx_clicks_url_id ON analytics.clicks (url_id);
CREATE INDEX idx_clicks_created_at ON analytics.clicks (created_at);
CREATE INDEX idx_clicks_ip ON analytics.clicks USING gist (ip inet_ops);
```

Monthly partitions:

```sql
CREATE TABLE analytics.clicks_2026_02 PARTITION OF analytics.clicks
    FOR VALUES FROM ('2026-02-01') TO ('2026-03-01');
```

Old partitions can be detached/dropped without vacuum overhead.

### Normalized tables — managed by consumer service

```sql
CREATE TABLE analytics.user_agents (
    id      SERIAL PRIMARY KEY,
    raw     TEXT NOT NULL UNIQUE
);

CREATE TABLE analytics.geo_locations (
    id      SERIAL PRIMARY KEY,
    country TEXT NOT NULL,       -- ISO 3166-1 alpha-2 (e.g. "RU", "US")
    city    TEXT,
    raw     TEXT NOT NULL UNIQUE -- original lookup key for deduplication
);

-- Normalized click view (consumer adds ua_id, referrer_domain, geo_id)
ALTER TABLE analytics.clicks ADD COLUMN ua_id INT REFERENCES analytics.user_agents(id);
ALTER TABLE analytics.clicks ADD COLUMN referrer_domain TEXT;
ALTER TABLE analytics.clicks ADD COLUMN geo_id INT REFERENCES analytics.geo_locations(id);
```

## Data Flow

### Phase 1: Direct insert (current plan)

```
Client → Redirect Handler → INSERT raw click into PG → Response
```

Simple synchronous insert in the redirect handler. At low traffic (+2-3ms per redirect) this is acceptable. Consumer service normalizes data asynchronously.

### Phase 2: RabbitMQ with in-memory buffer (when needed)

```
Client → Redirect Handler → push to buffer (Vec) → Response
                                       ↓
                            Background flush task
                         (every 500ms or 100 events)
                                       ↓
                         batch publish to RabbitMQ
                                       ↓
                              Consumer service
                                       ↓
                         Normalize + INSERT into PG
```

- Redirect handler pushes raw event into `Vec` behind `Mutex` — nanoseconds, zero impact on latency
- Background task flushes buffer to RabbitMQ by threshold (e.g. 100 events) or by timer (e.g. 500ms) — whichever comes first
- One batch = one Rabbit message with array of events, not per-click publishing
- Buffer memory footprint is negligible — 1000 clicks ~50 KB
- Consumer service handles normalization and writes to PG
- RabbitMQ is already in infrastructure

**Trade-off**: events in buffer are lost on crash. At low traffic and flush every 500ms — worst case loss is a few hundred clicks. Acceptable for analytics.

```rust
struct ClickBuffer {
    events: Mutex<Vec<RawClickEvent>>,
    flush_notify: Notify,
}

impl ClickBuffer {
    fn push(&self, event: RawClickEvent) {
        let mut events = self.events.lock().unwrap();
        events.push(event);
        if events.len() >= BATCH_SIZE {
            self.flush_notify.notify_one();
        }
    }
}

// Background task
async fn flush_loop(buffer: Arc<ClickBuffer>, rabbit: Channel) {
    loop {
        tokio::select! {
            _ = buffer.flush_notify.notified() => {},
            _ = tokio::time::sleep(FLUSH_INTERVAL) => {},
        }
        let batch = {
            let mut events = buffer.events.lock().unwrap();
            std::mem::take(&mut *events)
        };
        if !batch.is_empty() {
            rabbit.basic_publish(serialize_batch(&batch)).await;
        }
    }
}
```

## Click Event Structure (raw, from redirector)

| Field | Type | Source | Notes |
|-------|------|--------|-------|
| `id` | BIGINT | Snowflake ID | Generated by consumer, sortable by time |
| `url_id` | BIGINT | Decoded from hashid | Already available in handler |
| `ip` | INET | Request headers | `X-Forwarded-For` or `X-Real-IP` behind proxy |
| `user_agent` | TEXT | `User-Agent` header | Raw string, normalized by consumer |
| `referrer` | TEXT | `Referer` header | Raw value, parsed by consumer |
| `created_at` | TIMESTAMP | `now()` | Always UTC, no timezone overhead |

## Design Decisions

### ID: Snowflake instead of BIGSERIAL

Snowflake IDs are time-sortable, encode timestamp within the ID itself. Generated by the consumer service in Rust, not by PG sequences.

```rust
// 41 bit ms timestamp + 10 bit machine_id + 12 bit sequence
fn snowflake_id(machine_id: u16, seq: &AtomicU16) -> i64 {
    let epoch = 1_704_067_200_000; // 2024-01-01 UTC
    let ts = SystemTime::now()
        .duration_since(UNIX_EPOCH).unwrap()
        .as_millis() as i64 - epoch;
    let s = seq.fetch_add(1, Ordering::Relaxed) % 4096;
    (ts << 22) | ((machine_id as i64 & 0x3FF) << 12) | s as i64
}
```

### Timestamp: TIMESTAMP without timezone

Always UTC, no timezone conversion overhead. Both types are 8 bytes in PG, but `TIMESTAMP` is semantically cleaner when timezone is always known.

### IP: INET native type

4 bytes (IPv4) / 16 bytes (IPv6), native subnet operators (`>>=`), GiST indexable.

### GeoIP: MaxMind GeoLite2

Geographic location resolved by consumer at write time — not stored in raw click, not looked up at read time. This keeps the redirector fast and avoids bundling a 60+ MB database into the service.

- **Data source**: [MaxMind GeoLite2](https://dev.maxmind.com/geoip/geolite2-free-geolocation-data) (free, updated weekly)
- **Rust crate**: `maxminddb` — mmdb reader, zero-copy lookups
- **Lookup**: consumer resolves IP → country/city at INSERT time
- **Storage**: dictionary table `analytics.geo_locations` — same approach as UA dictionary, UNIQUE on raw lookup key to deduplicate
- **Update**: consumer periodically downloads fresh GeoLite2 database (weekly cron or on startup)

Geo data is a reference/dictionary table — the click row stores only a foreign key (`geo_id`), not the full location string.

### No extensions on PostgreSQL

All logic lives in the Rust consumer service — no TimescaleDB, no custom PG functions. Pure PostgreSQL:

- **Partition management** — consumer creates/drops monthly partitions
- **Retention** — consumer drops partitions older than N months
- **Compression** — PG TOAST handles strings automatically; if needed, consumer can pre-compress large fields before INSERT
- **UA dictionary** — consumer normalizes, not PG triggers
- **Referrer parsing** — consumer extracts domain before INSERT
- **GeoIP** — consumer resolves via MaxMind, not PG extension (e.g. no `ip4r`)

This keeps PG simple, portable, and free from extension dependencies.

## In-App Implementation (redirector only)

### Redirect handler change

```rust
// After resolving URL, before showing interstitial
analytics::record_click(url_id, &request).await;
```

### IP extraction

```rust
fn client_ip(headers: &HeaderMap, addr: SocketAddr) -> IpAddr {
    headers.get("x-forwarded-for")
        .and_then(|v| v.to_str().ok())
        .and_then(|s| s.split(',').next())
        .and_then(|s| s.trim().parse().ok())
        .unwrap_or(addr.ip())
}
```

## Configuration

```yaml
analytics:
  enabled: true
  # Phase 1
  storage: postgres        # or "rabbitmq" for Phase 2
  # Phase 2 (future)
  rabbitmq:
    url: ${RABBITMQ_URL}
    queue: redirector.clicks
    exchange: redirector
```

## Cleanup

Monthly partition management (DBA / consumer responsibility):

```sql
-- Detach old data (instant, no locks)
ALTER TABLE analytics.clicks DETACH PARTITION analytics.clicks_2025_01;

-- Drop when no longer needed
DROP TABLE analytics.clicks_2025_01;
```

## Out of Scope (for redirector)

- UA normalization / dictionary — consumer service
- Referrer domain extraction — consumer service
- GeoIP lookup (MaxMind) — consumer service
- Analytics dashboard — separate service
- Aggregation tables — consumer service
- Export to CSV/JSON — standard `COPY` from partitions
